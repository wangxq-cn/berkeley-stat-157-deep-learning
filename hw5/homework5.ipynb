{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 5 - Berkeley STAT 157\n",
    "\n",
    "**Name: Benson Yuan, SID 3031807403**\n",
    "\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/) instead of Github, so you will get the score distribution for each question. Please enroll in the [class](https://www.gradescope.com/courses/42432) by the Entry code: MXG5G5** \n",
    "\n",
    "Handout 2/19/2019, due 2/26/2019 by 4pm in Git by committing to your repository.\n",
    "\n",
    "In this homework, we will model covariate shift and attempt to fix it using logistic regression. This is a fairly realistic scenario for data scientists. To keep things well under control and understandable we will use [Fashion-MNIST](http://d2l.ai/chapter_linear-networks/fashion-mnist.html) as the data to experiment on. \n",
    "\n",
    "Follow the instructions from the Fashion MNIST notebook to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T19:52:59.411749Z",
     "start_time": "2019-02-12T19:52:28.081528Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_train = gdata.vision.FashionMNIST(train=True)\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "1. Implement the logistic loss function $l(y,f) = -\\log(1 + \\exp(-y f))$ in Gluon.\n",
    "2. Plot its values and its derivative for $y = 1$ and $f \\in [-5, 5]$, using automatic differentiation in Gluon.\n",
    "3. Generate training and test datasets for a binary classification problem using Fashion-MNIST with class $1$ being a combination of `shirt` and `sweater` and class $-1$ being the combination of `sandal` and `sneaker` categories. \n",
    "4. Train a binary classifier of your choice (it can be linear or a simple MLP such as from a previous lecture) using half the data (i.e. $12,000$ observations mixed as abvove) and one using the full dataset (i.e. $24,000$ observations as arising from the 4 categories) and report its accuracy. \n",
    "\n",
    "Hint - you should encapsulate the training and reporting code in a callable function since you'll need it quite a bit in the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Implement the logistic loss function $l(y,f) = -\\log(1 + \\exp(-y f))$ in Gluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l(y, f):\n",
    "    return -nd.log(1 + nd.exp(-y*f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Plot its values and its derivative for $y = 1$ and $f \\in [-5, 5]$, using automatic differentiation in Gluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEICAYAAAAgHpGBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6CjfvOwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o = nd.arange(-5, 6) \n",
    "o_cop = nd.arange(-5, 6) \n",
    "y_pos = nd.zeros(len(o)) \n",
    "pos_loss = l(1, o)\n",
    "\n",
    "o.attach_grad() \n",
    "with autograd.record():\n",
    "    pos_loss = l(1, o)\n",
    "pos_loss.backward()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(o.asnumpy(), pos_loss.asnumpy(), label = 'values') \n",
    "plt.plot(o.asnumpy(), o.grad.asnumpy(),label = 'grad')\n",
    "plt.title(\"Logistic loss function's values and its derivative for  y=1 and f∈[−5,5] ,\") \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Generate training and test datasets for a binary classification problem using Fashion-MNIST with class $1$ being a combination of `shirt` and `sweater` and class $-1$ being the combination of `sandal` and `sneaker` categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "\n",
    "def preprocess(data):\n",
    "    processed = []\n",
    "    for i in range(len(data)):\n",
    "        feature, label = data[i]\n",
    "        label = d2l.get_fashion_mnist_labels([label])[0]\n",
    "        if label == \"shirt\" or label == \"t-shirt\": # since there is no sweater, I chose t-shirt instead.\n",
    "            processed.append((data[i][0].astype('float32').reshape(1, 784), nd.array([1])))\n",
    "        elif label == \"sandal\" or label == \"sneaker\":\n",
    "            processed.append((data[i][0].astype('float32').reshape(1, 784), nd.array([-1])))\n",
    "    return processed\n",
    "\n",
    "new_train = preprocess(mnist_train)\n",
    "new_test = preprocess(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train) #sanity check that we get 24,000 observations as arising frmo the 4 categories. Good, it matches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Train a binary classifier of your choice (it can be linear or a simple MLP such as from a previous lecture) using half the data (i.e. $12,000$ observations mixed as abvove) and one using the full dataset (i.e. $24,000$ observations as arising from the 4 categories) and report its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First define some custom helper functions\n",
    "\n",
    "def init_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    return net\n",
    "\n",
    "def predict(y):\n",
    "    return 1 if y[0].asscalar() > 0 else -1 \n",
    "\n",
    "def train(train_iter, test_iter, loss, num_epochs, batch_size, lr=None):\n",
    "    net = init_net()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (predict(y_hat) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = get_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "    return net\n",
    "        \n",
    "def get_accuracy(data_iter, net):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum += (predict(net(X)) == y).sum()\n",
    "        n += y.size\n",
    "        #acc_sum.wait_to_read()\n",
    "    \n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on half of the dataset\n",
      "\n",
      "epoch 1, loss 211.7252, train acc 0.995, test acc 0.997\n",
      "epoch 2, loss 196.3562, train acc 0.998, test acc 0.997\n",
      "epoch 3, loss 63.7366, train acc 0.999, test acc 0.997\n",
      "epoch 4, loss 107.5051, train acc 0.999, test acc 0.998\n",
      "epoch 5, loss 61.9125, train acc 0.999, test acc 0.999\n",
      "Starting training on the full dataset\n",
      "epoch 1, loss 242.6006, train acc 0.996, test acc 0.997\n",
      "epoch 2, loss 193.8790, train acc 0.998, test acc 0.997\n",
      "epoch 3, loss 164.3251, train acc 0.999, test acc 0.998\n",
      "epoch 4, loss 148.6809, train acc 0.999, test acc 0.997\n",
      "epoch 5, loss 141.0904, train acc 0.999, test acc 0.998\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "loss = gloss.LogisticLoss()\n",
    "num_epochs, lr = 5, 0.5\n",
    "half = int(len(new_train)/2)\n",
    "halved = new_train[:half]\n",
    "print(\"Starting training on half of the dataset\")\n",
    "print()\n",
    "trained_half = train(halved, new_test, loss, num_epochs, batch_size, lr)\n",
    "print(\"Starting training on the full dataset\")\n",
    "trained_full = train(new_train, new_test, loss, num_epochs, batch_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariate Shift\n",
    "\n",
    "Your goal is to introduce covariate shit in the data and observe the accuracy.\n",
    "\n",
    "For this, compose a dataset of 12, 000 observations, given by a mixture of shirt and t-shirt and of sandal and sneaker respectively, where you use a fraction λ ∈ {0.05, 0.1, 0.2, . . . 0.8, 0.9, 0.95} of one (shirt and t-shirt) and a fraction of 1 − λ of the other datasets (sandal and sneaker) respectively.\n",
    "\n",
    "For instance, you might pick for λ = 0.1 a total of 600 shirt and 600 t-shirt images and likewise 5,400 sandal and 5, 400 sneaker photos, yielding a total of 12, 000 images for training. Note that the test set remains unbiased, composed of 2, 000 photos for the shirt + t-shirt category and of the sandal + sneaker category each.\n",
    "\n",
    "#### 1. Generate training sets that are appropriately biased. You should have 11 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_indices_of_label(labels):\n",
    "    indices_map = {}\n",
    "    for i in labels:\n",
    "        indices_map[i] = list()\n",
    "    for i in range(len(mnist_train)):\n",
    "        _, label = mnist_train[i]\n",
    "        if label in indices_map:\n",
    "            indices_map[label].append(i)\n",
    "    return indices_map \n",
    "\n",
    "# 0 = tshirt, 5 = sandal, 6 = shirt, 7 = sneaker.\n",
    "labels = [0, 5, 6, 7]\n",
    "indices_map = get_indices_of_label(labels) # indices map stores indices of each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(frac, indices_map):\n",
    "    num_per_class = len(indices_map[0])\n",
    "    size_class_one = int(frac * num_per_class)\n",
    "    size_class_two = int((1 - frac) * num_per_class)\n",
    "    tshirt_indices = np.random.choice(indices_map[0], size=size_class_one, replace = False)\n",
    "    shirt_indices = np.random.choice(indices_map[6], size=size_class_one, replace = False)\n",
    "    shirts_indices = np.concatenate((tshirt_indices, shirt_indices), axis=0)\n",
    "\n",
    "    sandal_indices = np.random.choice(indices_map[5], size=size_class_two, replace = False)\n",
    "    sneaker_indices = np.random.choice(indices_map[7], size=size_class_two, replace = False)\n",
    "    shoes_indices = np.concatenate((sandal_indices, sneaker_indices), axis=0)\n",
    "    \n",
    "    processed = []\n",
    "    for i in np.concatenate((shirts_indices, shoes_indices), axis=0):\n",
    "        feature, label = mnist_train[i]\n",
    "        if label == 0 or label == 6: # since there is no sweater, I chose t-shirt instead.\n",
    "            processed.append((mnist_train[i][0].astype('float32').reshape(1, 784), nd.array([1])))\n",
    "        elif label == 5 or label == 7:\n",
    "            processed.append((mnist_train[i][0].astype('float32').reshape(1, 784), nd.array([-1])))\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitted_data = []\n",
    "frac = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "for f in frac:\n",
    "    splitted_data.append(split_data(f, indices_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train a binary classifier using this and report the test set accuracy on the unbiased test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.8294, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 31.7829, train acc 0.999, test acc 0.507\n",
      "epoch 3, loss 25.9701, train acc 0.999, test acc 0.758\n",
      "epoch 1, loss 0.0217, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 33.0224, train acc 0.999, test acc 0.501\n",
      "epoch 3, loss 26.8802, train acc 0.999, test acc 0.779\n",
      "epoch 1, loss 0.3424, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 95.2141, train acc 0.999, test acc 0.713\n",
      "epoch 3, loss 11.6094, train acc 0.999, test acc 0.766\n",
      "epoch 1, loss 0.3038, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 78.3453, train acc 0.999, test acc 0.501\n",
      "epoch 3, loss 46.5414, train acc 0.999, test acc 0.794\n",
      "epoch 1, loss 0.0048, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 85.5992, train acc 0.999, test acc 0.501\n",
      "epoch 3, loss 63.4472, train acc 0.999, test acc 0.878\n",
      "epoch 1, loss 0.0748, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 61.7979, train acc 0.999, test acc 0.502\n",
      "epoch 3, loss 25.1434, train acc 1.000, test acc 0.748\n",
      "epoch 1, loss 5.9215, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 72.8818, train acc 0.999, test acc 0.513\n",
      "epoch 3, loss 43.3038, train acc 0.999, test acc 0.830\n",
      "epoch 1, loss 0.0027, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 78.8037, train acc 0.999, test acc 0.641\n",
      "epoch 3, loss 14.0137, train acc 0.999, test acc 0.772\n",
      "epoch 1, loss 3.9952, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 74.1785, train acc 0.999, test acc 0.701\n",
      "epoch 3, loss 29.8796, train acc 0.999, test acc 0.846\n",
      "epoch 1, loss 1.2095, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 36.5899, train acc 0.999, test acc 0.541\n",
      "epoch 3, loss 33.5396, train acc 0.999, test acc 0.783\n",
      "epoch 1, loss 3.7656, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 74.3138, train acc 0.999, test acc 0.749\n",
      "epoch 3, loss 43.4531, train acc 0.999, test acc 0.881\n"
     ]
    }
   ],
   "source": [
    "trained = []\n",
    "num_epochs, lr = 3, 0.5\n",
    "loss = gloss.LogisticLoss()\n",
    "for i, datum in enumerate(splitted_data):\n",
    "     trained.append(train(datum, new_test, loss, num_epochs, 256, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Covariate Shift Correction\n",
    "\n",
    "Having observed that covariate shift can be harmful, let's try fixing it. For this we first need to compute the appropriate propensity scores $\\frac{dp(x)}{dq(x)}$. For this purpose pick a biased dataset, let's say with $\\lambda = 0.1$ and try to fix the covariate shift.\n",
    "\n",
    "1. When training a logistic regression binary classifier to fix covariate shift, we assumed so far that both sets are of equal size. Show that re-weighting data in training and test set appropriately can help address the issue when both datasets have different size. What is the weighting?\n",
    "2. Train a binary classifier (using logistic regression) distinguishing between the biased training set and the unbiased test set. Note - you need to weigh the data. \n",
    "3. Use the scores to compute weights on the training set. Do they match the weight arising from the biasing distribution $\\lambda$? \n",
    "4. Train a binary classifier of the covariate shifted problem using the weights obtained previously and report the accuracy. Note - you will need to modify the training loop slightly such that you can compute the gradient of a weighted sum of losses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we want to estimate some dependency  p(y|x)  for which we have labeled data  (xi,yi). However, the observations  xi  are drawn from some distribution  q(x)  rather than the ‘proper’ distribution  p(x). We will show that regardless, we can find a proper weighting to reweight the data to correct covariate shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\int_{}^{} p(x)f(x) dx = \\int_{}^{} q(x)f(x) \\frac{p(x)}{q(x)} dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left, we want to model p(x)f(x), however, if covariate shift occurs, we only have access to q(x)f(x). However, we can estimate the weighting to be $\\beta(x) = \\frac{p(x)}{q(x)}$ which we will multiply with $ q(x)f(x)$ to get $p(x)f(x)$ to correct the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 6.3626, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 290.0754, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 251.4783, train acc 0.999, test acc 0.500\n"
     ]
    }
   ],
   "source": [
    "def train_with_weight(weight, train_iter, test_iter, loss, num_epochs, batch_size, lr=None):\n",
    "    net = init_net()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum() * weight\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (predict(y_hat) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = get_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "    return net\n",
    "\n",
    "biased_dataset = splitted_data[1]\n",
    "\n",
    "train_test_mix = []\n",
    "for i in range(len(biased_dataset)):\n",
    "    train_test_mix.append((biased_dataset[i][0].astype('float32').reshape(1, 784), nd.array([-1])))\n",
    "for i in range(len(new_test)):\n",
    "    train_test_mix.append((new_test[i][0].astype('float32').reshape(1, 784), nd.array([1])))\n",
    "    \n",
    "num_epochs, lr = 3, 0.5\n",
    "loss = gloss.LogisticLoss()\n",
    "result = train_with_weight(1/3, train_test_mix, new_test, loss, num_epochs, 256, lr) #2000/6000 = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we are able to achieve nearly 100% accuracy on the training set, which means that we are able to distinguish p(x) from q(x). Note that the test acc don't matter at this point because we are merely training the model in order to obtain B(x) = p(x)/q(x). (The test acc is merely an artifact from our regular classifier function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weigh training data using  $\\beta i$ = $min(exp(f(xi)),c)$ to clip the correction factor. In this case, we will pick c = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covariate_counter_train2(f, train_iter, test_iter, loss, num_epochs, batch_size, lr=None):\n",
    "    net = init_net()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = min(nd.exp(f(X)).sum().asscalar(), 100) * loss(y_hat, y).sum()  \n",
    "                #Above: we are reweighting by βi  = min(exp(f(xi)),100) to correct the covariate shift.\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (predict(y_hat) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = get_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.2838, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 7724.2362, train acc 0.999, test acc 0.510\n",
      "epoch 3, loss 3810.1495, train acc 0.999, test acc 0.754\n",
      "epoch 4, loss 3480.3880, train acc 1.000, test acc 0.896\n",
      "epoch 5, loss 1862.0473, train acc 1.000, test acc 0.956\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.5\n",
    "covariate_counter_train2(result, splitted_data[1], new_test, loss, num_epochs, 256, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 5.2141, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 95.7691, train acc 0.999, test acc 0.524\n",
      "epoch 3, loss 61.4966, train acc 0.999, test acc 0.831\n",
      "epoch 4, loss 23.1460, train acc 1.000, test acc 0.943\n",
      "epoch 5, loss 11.7775, train acc 1.000, test acc 0.837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dense(784 -> 1, linear)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(splitted_data[1], new_test, loss, num_epochs, 256, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great! After 5 epochs, we have 0.956 test accuracy after correction shift correction compared to 0.837 before the correction.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
