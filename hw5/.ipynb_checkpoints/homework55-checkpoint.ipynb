{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 5 - Berkeley STAT 157\n",
    "\n",
    "**Your name: Jack Sullivan, SID 25195800, teammates Jesse, Benson, Daiyaan, Mohameed** (Please add your name, SID and teammates to ease Ryan and Rachel to grade.)\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/) instead of Github, so you will get the score distribution for each question. Please enroll in the [class](https://www.gradescope.com/courses/42432) by the Entry code: MXG5G5** \n",
    "\n",
    "Handout 2/19/2019, due 2/26/2019 by 4pm in Git by committing to your repository.\n",
    "\n",
    "In this homework, we will model covariate shift and attempt to fix it using logistic regression. This is a fairly realistic scenario for data scientists. To keep things well under control and understandable we will use [Fashion-MNIST](http://d2l.ai/chapter_linear-networks/fashion-mnist.html) as the data to experiment on. \n",
    "\n",
    "Follow the instructions from the Fashion MNIST notebook to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T19:52:59.411749Z",
     "start_time": "2019-02-12T19:52:28.081528Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import numpy as np\n",
    "\n",
    "mnist_train = gdata.vision.FashionMNIST(train=True)\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "1. Implement the logistic loss function $l(y,f) = -\\log(1 + \\exp(-y f))$ in Gluon.\n",
    "2. Plot its values and its derivative for $y = 1$ and $f \\in [-5, 5]$, using automatic differentiation in Gluon.\n",
    "3. Generate training and test datasets for a binary classification problem using Fashion-MNIST with class $1$ being a combination of `shirt` and `sweater` and class $-1$ being the combination of `sandal` and `sneaker` categories. \n",
    "4. Train a binary classifier of your choice (it can be linear or a simple MLP such as from a previous lecture) using half the data (i.e. $12,000$ observations mixed as abvove) and one using the full dataset (i.e. $24,000$ observations as arising from the 4 categories) and report its accuracy. \n",
    "\n",
    "Hint - you should encapsulate the training and reporting code in a callable function since you'll need it quite a bit in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_loss(y, f):\n",
    "    return -1 * nd.log(1 + nd.exp(-y*f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_loss_arrays(y, f_s):\n",
    "    losses = []\n",
    "    for f in f_s:\n",
    "        losses.append(logistic_loss(y, f).asscalar())\n",
    "    return losses\n",
    "\n",
    "def gen_autograd_arrays(y, f_s):\n",
    "    losses = []\n",
    "    for f in f_s:\n",
    "        f.attach_grad()\n",
    "        with autograd.record():\n",
    "            result = logistic_loss(y, f)\n",
    "        result.backward()\n",
    "        losses.append(f.grad.asscalar()) \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_s = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "\n",
    "losses_y1 = gen_loss_arrays(1, nd.array(f_s))\n",
    "grads_y1 = gen_autograd_arrays(1, nd.array(f_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(f_s, losses_y1, label=\"values\")\n",
    "plt.plot(f_s, grads_y1, label=\"derivative\")\n",
    "plt.xlabel(\"f\")\n",
    "plt.ylabel(\"Value or Derivative\")\n",
    "plt.title(\"Value or Derivative vs f\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "\n",
    "def get_data(d):\n",
    "    new_data = []\n",
    "    for i in range(len(d)):\n",
    "        feature, label = d[i]\n",
    "        label = d2l.get_fashion_mnist_labels([label])[0]\n",
    "        if label == \"shirt\" or label == \"t-shirt\":\n",
    "            new_data.append((d[i][0].astype('float32').reshape(1, 784), nd.array([1])))\n",
    "        elif label == \"sandal\" or label == \"sneaker\":\n",
    "            new_data.append((d[i][0].astype('float32').reshape(1, 784), nd.array([-1])))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_train = get_data(mnist_train)\n",
    "new_test = get_data(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 4000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train), len(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(yhat):\n",
    "    if yhat[0].asscalar() > 0:\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = gloss.LogisticLoss()\n",
    "num_epochs, lr = 5, 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, loss, num_epochs, batch_size, lr=None):\n",
    "    net = get_net()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {\n",
    "        'learning_rate': lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (predict(y_hat) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "    return net\n",
    "        \n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum += (predict(net(X)) == y).sum()\n",
    "        n += y.size\n",
    "        #acc_sum.wait_to_read()\n",
    "    \n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 211.7215, train acc 0.995, test acc 0.997\n",
      "epoch 2, loss 196.3491, train acc 0.998, test acc 0.997\n",
      "epoch 3, loss 63.7380, train acc 0.999, test acc 0.997\n",
      "epoch 4, loss 107.5033, train acc 0.999, test acc 0.998\n",
      "epoch 5, loss 61.9039, train acc 0.999, test acc 0.999\n"
     ]
    }
   ],
   "source": [
    "batch_size = len(new_train) / 2\n",
    "net_half = train(new_train[:int(len(new_train)/2)], new_test, loss, num_epochs, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 242.5977, train acc 0.996, test acc 0.997\n",
      "epoch 2, loss 193.8834, train acc 0.998, test acc 0.997\n",
      "epoch 3, loss 164.3264, train acc 0.999, test acc 0.998\n",
      "epoch 4, loss 148.6485, train acc 0.999, test acc 0.997\n",
      "epoch 5, loss 141.0537, train acc 0.999, test acc 0.998\n"
     ]
    }
   ],
   "source": [
    "batch_size = len(new_train)\n",
    "net_full = train(new_train, new_test, loss, num_epochs, batch_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariate Shift\n",
    "\n",
    "Your goal is to introduce covariate shit in the data and observe the accuracy. For this, compose a dataset of $12,000$ observations, given by a mixture of `shirt` and `sweater` and of `sandal` and `sneaker` respectively, where you use a fraction $\\lambda \\in \\{0.05, 0.1, 0.2, \\ldots 0.8, 0.9, 0.95\\}$ of one and a fraction of $1-\\lambda$ of  the other datasets respectively. For instance, you might pick for $\\lambda = 0.1$ a total of $600$ `shirt` and $5,400$ `sweater` images and likewise $600$ `sandal` and $5,400$ `sneaker` photos, yielding a total of $12,000$ images for training. Note that the test set remains unbiased, composed of $2,000$ photos for the `shirt` + `sweater` category and of the `sandal` + `sneaker` category each.\n",
    "\n",
    "1. Generate training sets that are appropriately biased. You should have 11 datasets.\n",
    "2. Train a binary classifier using this and report the test set accuracy on the unbiased test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "label_map[0] = list()\n",
    "label_map[6] = list()\n",
    "label_map[5] = list()\n",
    "label_map[7] = list()\n",
    "\n",
    "for i in range(len(mnist_train)):\n",
    "    _, label = mnist_train[i]\n",
    "    if label in label_map:\n",
    "        label_map[label].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_unbal_data(l):\n",
    "    tshirt_indices = np.random.choice(label_map[0], int(len(label_map[0])*l), replace=False)\n",
    "    shirt_indices = np.random.choice(label_map[6], int(len(label_map[6])*(l)), replace=False)\n",
    "    shirts_indices = np.concatenate((tshirt_indices, shirt_indices), axis=0)\n",
    "    \n",
    "    sandal_indices = np.random.choice(label_map[5], int(len(label_map[5])*(1-l)), replace=False)\n",
    "    sneaker_indices = np.random.choice(label_map[7], int(len(label_map[7])*(1-l)), replace=False)\n",
    "    shoes_indices = np.concatenate((sandal_indices, sneaker_indices), axis=0)\n",
    "    \n",
    "    new_data = []\n",
    "    for i in np.concatenate((shirts_indices, shoes_indices), axis=0):\n",
    "        feature, label = mnist_train[i]\n",
    "        if label == 0 or label == 6:\n",
    "            new_data.append((mnist_train[i][0].astype('float32').reshape(1, 784), nd.array([1])))\n",
    "        elif label == 5 or label == 7:\n",
    "            new_data.append((mnist_train[i][0].astype('float32').reshape(1, 784), nd.array([-1])))\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_5 = gen_unbal_data(0.05)\n",
    "l_10 = gen_unbal_data(0.1)\n",
    "l_20 = gen_unbal_data(0.2)\n",
    "l_30 = gen_unbal_data(0.3)\n",
    "l_40 = gen_unbal_data(0.4)\n",
    "l_50 = gen_unbal_data(0.5)\n",
    "l_60 = gen_unbal_data(0.6)\n",
    "l_70 = gen_unbal_data(0.7)\n",
    "l_80 = gen_unbal_data(0.8)\n",
    "l_90 = gen_unbal_data(0.9)\n",
    "l_95 = gen_unbal_data(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 12.1200, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 137.3798, train acc 0.999, test acc 0.596\n",
      "epoch 3, loss 50.3621, train acc 0.999, test acc 0.732\n",
      "epoch 4, loss 27.3224, train acc 0.999, test acc 0.808\n",
      "epoch 5, loss 87.3855, train acc 0.999, test acc 0.935\n",
      "epoch 1, loss 4.6575, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 109.3837, train acc 0.999, test acc 0.573\n",
      "epoch 3, loss 17.6157, train acc 1.000, test acc 0.858\n",
      "epoch 4, loss 6.4145, train acc 1.000, test acc 0.792\n",
      "epoch 5, loss 46.4805, train acc 1.000, test acc 0.861\n",
      "epoch 1, loss 3.2561, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 70.4955, train acc 0.999, test acc 0.504\n",
      "epoch 3, loss 72.8226, train acc 0.999, test acc 0.711\n",
      "epoch 4, loss 51.0601, train acc 0.999, test acc 0.763\n",
      "epoch 5, loss 35.7341, train acc 1.000, test acc 0.802\n",
      "epoch 1, loss 6.6458, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 171.7697, train acc 0.999, test acc 0.674\n",
      "epoch 3, loss 62.7522, train acc 0.999, test acc 0.786\n",
      "epoch 4, loss 24.9059, train acc 0.999, test acc 0.805\n",
      "epoch 5, loss 54.9052, train acc 0.999, test acc 0.820\n",
      "epoch 1, loss 7.3745, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 172.4622, train acc 0.999, test acc 0.596\n",
      "epoch 3, loss 11.0264, train acc 1.000, test acc 0.796\n",
      "epoch 4, loss 62.6937, train acc 1.000, test acc 0.894\n",
      "epoch 5, loss 40.0444, train acc 1.000, test acc 0.992\n",
      "epoch 1, loss 0.0102, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 165.5036, train acc 0.999, test acc 0.501\n",
      "epoch 3, loss 80.4470, train acc 0.999, test acc 0.708\n",
      "epoch 4, loss 48.0455, train acc 1.000, test acc 0.807\n",
      "epoch 5, loss 31.0936, train acc 1.000, test acc 0.907\n",
      "epoch 1, loss 6.6540, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 161.7576, train acc 0.999, test acc 0.507\n",
      "epoch 3, loss 93.0691, train acc 0.999, test acc 0.832\n",
      "epoch 4, loss 22.3128, train acc 0.999, test acc 0.843\n",
      "epoch 5, loss 152.6929, train acc 0.999, test acc 0.901\n",
      "epoch 1, loss 0.0055, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 129.3769, train acc 0.999, test acc 0.502\n",
      "epoch 3, loss 64.3954, train acc 0.999, test acc 0.793\n",
      "epoch 4, loss 87.1528, train acc 0.999, test acc 0.870\n",
      "epoch 5, loss 84.0573, train acc 0.999, test acc 0.987\n",
      "epoch 1, loss 9.9493, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 132.9743, train acc 0.999, test acc 0.610\n",
      "epoch 3, loss 40.4833, train acc 0.999, test acc 0.817\n",
      "epoch 4, loss 87.9164, train acc 0.999, test acc 0.935\n",
      "epoch 5, loss 22.1144, train acc 1.000, test acc 0.996\n",
      "epoch 1, loss 9.5048, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 224.7490, train acc 0.999, test acc 0.826\n",
      "epoch 3, loss 55.8482, train acc 1.000, test acc 0.918\n",
      "epoch 4, loss 31.4080, train acc 0.999, test acc 0.968\n",
      "epoch 5, loss 45.1232, train acc 0.999, test acc 0.925\n",
      "epoch 1, loss 10.3623, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 214.0569, train acc 0.999, test acc 0.525\n",
      "epoch 3, loss 87.2967, train acc 0.999, test acc 0.873\n",
      "epoch 4, loss 112.1081, train acc 0.999, test acc 0.943\n",
      "epoch 5, loss 46.8364, train acc 0.999, test acc 0.995\n"
     ]
    }
   ],
   "source": [
    "net_l_5 = train(l_5, new_test, loss, num_epochs, len(l_5), lr)\n",
    "net_l_10 = train(l_10, new_test, loss, num_epochs, len(l_10), lr)\n",
    "net_l_20 = train(l_20, new_test, loss, num_epochs, len(l_20), lr)\n",
    "net_l_30 = train(l_30, new_test, loss, num_epochs, len(l_30), lr)\n",
    "net_l_40 = train(l_40, new_test, loss, num_epochs, len(l_40), lr)\n",
    "net_l_50 = train(l_50, new_test, loss, num_epochs, len(l_50), lr)\n",
    "net_l_60 = train(l_60, new_test, loss, num_epochs, len(l_60), lr)\n",
    "net_l_70 = train(l_70, new_test, loss, num_epochs, len(l_70), lr)\n",
    "net_l_80 = train(l_80, new_test, loss, num_epochs, len(l_80), lr)\n",
    "net_l_90 = train(l_90, new_test, loss, num_epochs, len(l_90), lr)\n",
    "net_l_95 = train(l_95, new_test, loss, num_epochs, len(l_95), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Covariate Shift Correction\n",
    "\n",
    "Having observed that covariate shift can be harmful, let's try fixing it. For this we first need to compute the appropriate propensity scores $\\frac{dp(x)}{dq(x)}$. For this purpose pick a biased dataset, let's say with $\\lambda = 0.1$ and try to fix the covariate shift.\n",
    "\n",
    "1. When training a logistic regression binary classifier to fix covariate shift, we assumed so far that both sets are of equal size. Show that re-weighting data in training and test set appropriately can help address the issue when both datasets have different size. What is the weighting?\n",
    "2. Train a binary classifier (using logistic regression) distinguishing between the biased training set and the unbiased test set. Note - you need to weigh the data. \n",
    "3. Use the scores to compute weights on the training set. Do they match the weight arising from the biasing distribution $\\lambda$? \n",
    "4. Train a binary classifier of the covariate shifted problem using the weights obtained previously and report the accuracy. Note - you will need to modify the training loop slightly such that you can compute the gradient of a weighted sum of losses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data = []\n",
    "for i in range(len(l_10)):\n",
    "    pic, _ = l_10[i]\n",
    "    train_test_data.append((pic, nd.array([-1])))\n",
    "for i in range(len(new_test)):\n",
    "    pic, _ = new_test[i]\n",
    "    train_test_data.append((pic, nd.array([1])))\n",
    "len(train_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 18.8211, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 713.5405, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 639.9972, train acc 0.999, test acc 0.500\n",
      "epoch 4, loss 736.5134, train acc 0.999, test acc 0.500\n",
      "epoch 5, loss 603.2312, train acc 0.999, test acc 0.500\n"
     ]
    }
   ],
   "source": [
    "f = train(train_test_data, new_test, loss, num_epochs, len(train_test_data), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def c_shift_train(f, train_iter, test_iter, loss, num_epochs, batch_size, lr=None):\n",
    "    net = get_net()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {\n",
    "        'learning_rate': lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = min(nd.exp(f(X)).sum().asscalar(), 100)*loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (predict(y_hat) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 647.3896, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 7679.1511, train acc 0.999, test acc 0.504\n",
      "epoch 3, loss 3788.5859, train acc 0.999, test acc 0.762\n",
      "epoch 4, loss 3320.6388, train acc 0.999, test acc 0.909\n",
      "epoch 5, loss 1765.5310, train acc 1.000, test acc 0.969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dense(784 -> 1, linear)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_shift_train(f, l_10, new_test, loss, num_epochs, len(l_10), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier works about 10% better than before! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
